{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"CSE327-HW2-Spring20.ipynb","provenance":[{"file_id":"1GVHyf_W0MeVtydfCYyguPcW32LQpnSwi","timestamp":1568946615231},{"file_id":"14wlPKdLdRnHCjQQccsUxaq_cqEG719L-","timestamp":1568923971871}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"OZyBuMSqe542","colab_type":"text"},"source":["# CSE327 Homework2\n","**Due date: 23:59 on March 12, 2020 (Thursday)**\n","\n","---\n","In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n","\n","## Google Colab Tutorial\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n","\n","\n","## Local Machine Prerequisites\n","---\n","Since we are using Google Colab, all the code is run on the server environment where lots of libraries or packages have already been installed. In case of missing \n"," libraries or if you want to install them in your local machine, below are the links for installation.\n","* **Install Python 3.6**: https://www.python.org/downloads/ or use Anaconda (a Python distribution) at https://docs.continuum.io/anaconda/install/. Below are some materials and tutorials which you may find useful for learning Python if you are new to Python.\n","  - https://docs.python.org/3.6/tutorial/index.html\n","  - https://www.learnpython.org/\n","  - http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n","  - http://www.scipy-lectures.org/advanced/image_processing/index.html\n","\n","\n","* **Install Python packages**: install Python packages: `numpy`, `matplotlib`, `opencv-python` using pip, for example:\n","```\n","pip install numpy matplotlib opencv-python\n","``` \n","\tNote that when using “pip install”, make sure that the version you are using is python3. Below are some commands to check which python version it uses in you machine. You can pick one to execute:\n","  \n","```  \n","    pip show pip\n","\n","    pip --version\n","\n","    pip -V\n","\n","```\n","\n","Incase of wrong version, use pip3 for python3 explictly.\n","\n","* **Install Jupyter Notebook**: follow the instructions at http://jupyter.org/install.html to install Jupyter Notebook and familiarize yourself  with it. *After you have installed Python and Jupyter Notebook, please open the notebook file 'HW1.ipynb' with your Jupyter Notebook and do your homework there.*\n","\n","## Description\n","---\n","In this homework you will experiment with SIFT features for scene matching and object recognition. You will work with the SIFT tutorial and code from the University of Toronto. In the compressed homework file, you will find the tutorial document (tutSIFT04.pdf) and a paper from the International Journal of Computer Vision (ijcv04.pdf) describing SIFT and object recognition. Although the tutorial document assumes matlab implemention, you should still be able to follow the technical details in it. In addition, you are **STRONGLY** encouraged to read this paper unless you’re already quite familiar with matching and recognition using SIFT.\n","\n","There are 2 problems in this homework with a total of 100 points. Be sure to read **Submission Guidelines** below. They are important.\n","\n","\n","\n","## Using SIFT in OpenCV 3.x.x in Local Machine\n","---\n","Feature descriptors like SIFT and SURF are no longer included in OpenCV since version 3. This section provides instructions on how to use SIFT for those who use OpenCV 3.x.x. If you are using OpenCV 2.x.x then you are all set, please skip this section. Read this if you are curious about why SIFT is removed https://www.pyimagesearch.com/2015/07/16/where-did-sift-and-surf-go-in-opencv-3/.\n","\n","**We strongly recommend you to use SIFT methods in Colab for this homework**, the details will be described in the next section.\n","\n","However, if you want to use SIFT in your local machine, one simple way to use the OpenCV in-built function `SIFT` is to switch back to version 2.x.x, but if you want to keep using OpenCV 3.x.x, do the following:\n","1. uninstall your original OpenCV package\n","2. install opencv-contrib-python using pip (pip is a Python tool for installing packages written in Python), please find detailed instructions at https://pypi.python.org/pypi/opencv-contrib-python\n","\n","After you have your OpenCV set up, you should be able to use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n","\n","## Using SIFT in OpenCV 3.x.x in Colab (RECOMMENDED)\n","---\n","The default version of OpenCV in Colab is 3.4.3. If we use SIFT method directly, typically we will get this error message:\n","\n","```\n","error: OpenCV(3.4.3) /io/opencv_contrib/modules/xfeatures2d/src/sift.cpp:1207: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n","\n","```\n","\n","One simple way to use the OpenCV in-built function `SIFT` in Colab is to switch the version to the one from 'contrib'. Below is an example of switching OpenCV version:\n","\n","1. Run the following command in one section in Colab, which has already been included in this assignment:\n","```\n","pip install opencv-contrib-python==3.4.2.16\n","```\n","2. Restart runtime by\n","```\n","Runtime -> Restart Runtime\n","```\n","\n","Then you should be able to use use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n","\n","## Some Resources\n","---\n","In addition to the tutorial document, the following resources can definitely help you in this homework:\n","- http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n","- http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html\n","- http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html?highlight=sift#cv2.SIFT\n","- http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html"]},{"cell_type":"code","metadata":{"id":"omyazyywtKQS","colab_type":"code","outputId":"d271486c-0e24-432e-c4f9-2e964ee4937a","executionInfo":{"status":"ok","timestamp":1586232398789,"user_tz":240,"elapsed":8947,"user":{"displayName":"Pavani Tripathi","photoUrl":"","userId":"03125543765874684009"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["# pip install the OpenCV version from 'contrib'\n","pip install opencv-contrib-python==3.4.2.16"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting opencv-contrib-python==3.4.2.16\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/f1/66330f4042c4fb3b2d77a159db8e8916d9cdecc29bc8c1f56bc7f8a9bec9/opencv_contrib_python-3.4.2.16-cp36-cp36m-manylinux1_x86_64.whl (30.6MB)\n","\u001b[K     |████████████████████████████████| 30.6MB 132kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==3.4.2.16) (1.18.2)\n","Installing collected packages: opencv-contrib-python\n","  Found existing installation: opencv-contrib-python 4.1.2.30\n","    Uninstalling opencv-contrib-python-4.1.2.30:\n","      Successfully uninstalled opencv-contrib-python-4.1.2.30\n","Successfully installed opencv-contrib-python-3.4.2.16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wnIVJsktWz5-","colab_type":"code","outputId":"0083c787-47b8-4e5c-86de-6e349d9c5da2","executionInfo":{"status":"ok","timestamp":1586232398790,"user_tz":240,"elapsed":8936,"user":{"displayName":"Pavani Tripathi","photoUrl":"","userId":"03125543765874684009"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# import packages here\n","import cv2\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","print(cv2.__version__) # verify OpenCV version"],"execution_count":2,"outputs":[{"output_type":"stream","text":["3.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DRJ7cx71t8XR","colab_type":"code","outputId":"d079135f-ca8f-496f-a193-52224d226f00","executionInfo":{"status":"ok","timestamp":1584311218409,"user_tz":240,"elapsed":40915,"user":{"displayName":"Nur Nibir","photoUrl":"","userId":"02931620758331555754"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Mount your google drive where you've saved your assignment folder\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DYCDG18Ht50P","colab_type":"code","colab":{}},"source":["# Set your working directory (in your google drive)\n","#   change it to your specific homework directory.\n","#/content/gdrive/My Drive/CSE327-HW2-Spring20\n","cd '/content/gdrive/My Drive/Nibir_Nur_111059470_hw2/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mROKmAhve545","colab_type":"text"},"source":["## Problem 1: Match transformed images using SIFT features\n","{60 points} You will transform a given image, and match it back to the original image using SIFT keypoints.\n","\n","- **Step 1 (10pt)**. Use the function from SIFT class to detect keypoints from the given image. Plot the image with keypoints scale and orientation overlaid.\n","\n","- **Step 2 (10pt)**. Rotate your image clockwise by 60 degrees with the `cv2.warpAffine` function. Extract SIFT keypoints for this rotated image and plot the rotated picture with keypoints scale and orientation overlaid just as in step 1.\n","\n","- **Step 3 (15pt)**. Match the SIFT keypoints of the original image and the rotated imag using the `knnMatch` function in the `cv2.BFMatcher` class. Discard bad matches using the ratio test proposed by D.Lowe in the SIFT paper. Use **0.1** as the ratio in this homework. Note that this is for display purpose only. Draw the filtered good keypoint matches on the image and display it. The image you draw should have two images side by side with matching lines across them.\n","\n","- **Step 4 (15pt)**. Use the RANSAC algorithm to find the affine transformation from the rotated image to the original image. You are not required to implement the RANSAC algorithm yourself, instead you could use the `cv2.findHomography` function (set the 3rd parameter `method` to `cv2.RANSAC`) to compute the transformation matrix. Transform the rotated image back using this matrix and the `cv2.warpPerspective` function. Display the recovered image.\n","\n","- **Step 5 (10pt)**. You might have noticed that the rotated image from step 2 is cropped. Rotate the image without any cropping and you will be awarded an extra 5 points.\n","\n","Hints: In case of too many matches in the output image, use the ratio of 0.1 to filter matches."]},{"cell_type":"code","metadata":{"id":"tn4AgU_lLp6_","colab_type":"code","colab":{}},"source":["def drawMatches(img1, kp1, img2, kp2, matches):\n","    \"\"\"\n","    My own implementation of cv2.drawMatches as OpenCV 2.4.9\n","    does not have this function available but it's supported in\n","    OpenCV 3.0.0\n","\n","    This function takes in two images with their associated \n","    keypoints, as well as a list of DMatch data structure (matches) \n","    that contains which keypoints matched in which images.\n","\n","    An image will be produced where a montage is shown with\n","    the first image followed by the second image beside it.\n","\n","    Keypoints are delineated with circles, while lines are connected\n","    between matching keypoints.\n","\n","    img1,img2 - Grayscale images\n","    kp1,kp2 - Detected list of keypoints through any of the OpenCV keypoint \n","              detection algorithms\n","    matches - A list of matches of corresponding keypoints through any\n","              OpenCV keypoint matching algorithm\n","    \"\"\"\n","\n","    # Create a new output image that concatenates the two images together\n","    # (a.k.a) a montage\n","    rows1 = img1.shape[0]\n","    cols1 = img1.shape[1]\n","    rows2 = img2.shape[0]\n","    cols2 = img2.shape[1]\n","\n","    # Create the output image\n","    # The rows of the output are the largest between the two images\n","    # and the columns are simply the sum of the two together\n","    # The intent is to make this a colour image, so make this 3 channels\n","    out = np.zeros((max([rows1,rows2]),cols1+cols2,3), dtype='uint8')\n","\n","    # Place the first image to the left\n","    out[:rows1,:cols1] = np.dstack([img1, img1, img1])\n","\n","    # Place the next image to the right of it\n","    out[:rows2,cols1:] = np.dstack([img2, img2, img2])\n","\n","    # For each pair of points we have between both images\n","    # draw circles, then connect a line between them\n","    for mat in matches:\n","\n","        # Get the matching keypoints for each of the images\n","        img1_idx = mat.queryIdx\n","        img2_idx = mat.trainIdx\n","\n","        # x - columns\n","        # y - rows\n","        (x1,y1) = kp1[img1_idx].pt\n","        (x2,y2) = kp2[img2_idx].pt\n","\n","        # Draw a small circle at both co-ordinates\n","        # radius 4\n","        # colour blue\n","        # thickness = 1\n","        cv2.circle(out, (int(x1),int(y1)), 4, (255, 0, 0), 1)   \n","        cv2.circle(out, (int(x2)+cols1,int(y2)), 4, (255, 0, 0), 1)\n","\n","        # Draw a line in between the two points\n","        # thickness = 1\n","        # colour blue\n","        cv2.line(out, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), (0,255,0), 2)\n","\n","    # Also return the image if you'd like a copy\n","    return out\n","\n","# Read image\n","img_input = cv2.imread('SourceImages/sift_input.JPG', 0)\n","\n","# initiate SIFT detector\n","sift = cv2.xfeatures2d.SIFT_create()\n","\n","# find the keypoints and descriptors with SIFT\n","kp, des = sift.detectAndCompute(img_input,None)\n","\n","# Darw keypoints on the image\n","# ===== This is your first output ===== \n","res1 = cv2.drawKeypoints(img_input, kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","\n","# rotate image\n","inputH, inputW = img_input.shape\n","M = cv2.getRotationMatrix2D((inputW/2, inputH/2), -60, 1)\n","rotCos = abs(M[0][0]) \n","rotSin = abs(M[0][1])\n","\n","newH = int(inputH * rotCos + inputW * rotSin)\n","newW = int(inputH * rotSin + inputW * rotCos)\n","\n","M[0][2] += newW/2 - inputW/2\n","M[1][2] += newH/2 - inputH/2\n","\n","rotatedImg = cv2.warpAffine(img_input, M, (newW, newH))\n","\n","# find the keypoints and descriptors on the rotated image\n","kpRotated, desRotated = sift.detectAndCompute(rotatedImg, None)\n","\n","# Darw keypoints on the rotated image\n","# ===== This is your second output =====\n","res2 = cv2.drawKeypoints(rotatedImg, kpRotated, None, flags=cv2.cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","\n","# ====== Plot functions, DO NOT CHANGE =====\n","# Plot result images\n","plt.figure(figsize=(12,8))\n","plt.subplot(1, 2, 1)\n","plt.imshow(res1, 'gray')\n","plt.title('original img')\n","plt.axis('off')\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(res2, 'gray')\n","plt.title('rotated img')\n","plt.axis('off')\n","# ==========================================\n","\n","# compute feature matching\n","bf = cv2.BFMatcher()\n","matches = bf.knnMatch(des,desRotated, k=2)\n","\n","# Apply ratio test\n","good_matches = [] # Append filtered matches to this list\n","for i, j in matches:\n","  if i.distance < 0.1*j.distance:\n","    good_matches.append(i)\n","\n","# draw matching results with the given drawMatches function\n","# ===== This is your third output =====\n","res3 = drawMatches(img_input, kp, rotatedImg, kpRotated, good_matches)\n","\n","# ====== Plot functions, DO NOT CHANGE =====\n","plt.figure(figsize=(12,8))\n","plt.imshow(res3)\n","plt.title('matching')\n","plt.axis('off')\n","# ==========================================\n","\n","# estimate similarity transform\n","if len(good_matches) > 4:\n","    src_points = []\n","    des_points = []\n","     #find perspective transform matrix using RANSAC\n","    for match in good_matches:\n","      src_points.append(kp[match.queryIdx].pt)\n","      des_points.append(kpRotated[match.trainIdx].pt)\n","  \n","    src_points = np.float32(src_points).reshape(-1,1,2)\n","    des_points = np.float32(des_points).reshape(-1,1,2)\n","\n","    rot, mask = cv2.findHomography(des_points, src_points, cv2.RANSAC)\n","    print(\"Transformation Matrix = \\n\", rot)\n","  \n","    # mapping rotataed image back with the calculated rotation matrix\n","    # ===== This is your fourth output =====\n","    h,w = img_input.shape \n","    res4 = cv2.warpPerspective(rotatedImg, rot, (w, h))\n","else:\n","    print(\"Not enough matches are found - %d/%d\" % (len(good_matches),4))\n","\n","# ====== Plot functions, DO NOT CHANGE =====\n","# plot result images\n","plt.figure(figsize=(12,8))\n","plt.subplot(1, 2, 1)\n","plt.imshow(img_input, 'gray')\n","plt.title('original img')\n","plt.axis('off')\n","    \n","plt.subplot(1, 2, 2)\n","plt.imshow(res4, 'gray')\n","plt.title('recovered img')  \n","plt.axis('off')\n","# =========================================="],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQdh9GvXe54_","colab_type":"text"},"source":["## Problem 2: Object Recognition with HOG features\n","{40 points} You will use the histogram of oriented gradients (HOG) to extract features from objects and recognize them.\n","\n","HOG decomposes an image into multiple cells, computes the direction of the gradients for all pixels in each cell, and creates a histogram of gradient orientation for that cell. Object recognition with HOG is usually done by extracting HOG features from a training set of images, learning a support vector machine (SVM) from those features, and then testing a new image with the SVM to determine the existence of an object.\n","\n","You can use `cv2.HOGDescriptor` to extract the HoG feature and `cv2.ml.SVM_create` for SVMs (and a lot of other algorithms). You can also use Python machine learning packages for SVM, e.g.`scikit-learn` and for HoG computation, e.g. `scikit-image`. Please find the OpenCV SVM tutorial at https://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/.\n","\n","An image set located under SourceImages/human_vs_birds is provided containing 20 images. You will first train an SVM with the HoG features and then predict the class of an image with the trained SVM. For simplicity, we will be dealing with a binary classification problem with two classes, namely, birds and humans. There are 10 images for each class.\n","\n","Some of the function names and arguments are provided, you may change them as you see fit.\n","\n","- **Step 1 (5pts)**. Load in the images and create a vector of corresponding labels (0 for bird and 1 for human). An example label vector should be something like [1,1,1,1,1,0,0,0,0,0]. Shuffle the images randomly and display them in a 2 x 10 grid with figsize = (18, 15).\n","\n","- **Step 2 (15pts)**. Extract HoG features from all images. You can use the OpenCV function `cv2.HOGDescriptor` or hog routine from `scikit-image`. Display the HoG features for all images in a 2 x 10 grid with figsize = (18, 15).\n","\n","- **Step 3**. Use the first 16 examples from the shuffled dataset as training data on which to train an SVM. The rest 4 are used as test data. Reshape the HoG feature matrix as necessary to feed into the SVM. Train the classifier. **DO NOT train with test data.** No output is expected from this part.\n","\n","- **Step 4 (20pts)**. Perform predictions with your trained SVM on the test data. Output a vector of predictions, a vector of ground truth labels, and prediction accuracy."]},{"cell_type":"code","metadata":{"id":"OqSWkI8hL4o0","colab_type":"code","colab":{}},"source":["import os\n","import skimage.exposure\n","from skimage.feature import hog\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# load data\n","def loadData(filePath, endName):\n","    # Implement your loadData(file) here\n","    dataLst = []\n","    for fileName in os.listdir(filePath):\n","      if endName in fileName:\n","        dataImg = cv2.imread(filePath + fileName)\n","        dataLst.append(dataImg)\n","    return dataLst\n","\n","birdImgs = loadData('SourceImages/human_vs_birds/', 'bird_')\n","humanImgs = loadData('SourceImages/human_vs_birds/', 'human_')\n","\n","combinedArr = np.concatenate((birdImgs, humanImgs))\n","\n","randomState = np.random.get_state()\n","np.random.shuffle(combinedArr)\n","\n","# ===== Display your first graph here =====\n","plt.figure(figsize=(18,15))\n","for i in range(20):\n","  plt.subplot(2, 10, i+1)\n","  plt.imshow(cv2.cvtColor(combinedArr[i], cv2.COLOR_BGR2RGB))\n","  plt.axis('off')\n","\n","# create a vector of labels\n","vector = [0 for i in birdImgs]\n","vector.extend([1 for i in humanImgs])\n","np.random.set_state(randomState)\n","np.random.shuffle(vector)\n","\n","# assume labels: bird = 0, human = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUAT1obsL836","colab_type":"code","colab":{}},"source":["# Compute HOG features for the images\n","def computeHOGfeatures(img):\n","    # Implement your computeHOGfeatures() here\n","    features, HoGImage = hog(img, \n","                             orientations=8, \n","                             pixels_per_cell=(16, 16), \n","                             cells_per_block=(1, 1), \n","                             visualize=True, \n","                             feature_vector=True)\n","    return features, HoGImage\n","\n","# Compute HOG descriptors\n","features = []\n","HoGImages = []\n","for i in range(len(combinedArr)):\n","  featureVector, HoGImage = computeHOGfeatures(combinedArr[i])\n","  features.append(featureVector)\n","  HoGImage = skimage.exposure.rescale_intensity(HoGImage)\n","  HoGImages.append(HoGImage)\n","\n","# ===== Display your second graph here =====\n","plt.figure(figsize=(18,15))\n","for i in range(20):\n","  plt.subplot(2, 10, i+1)\n","  plt.imshow(HoGImages[i], 'gray')\n","  plt.axis('off')\n","\n","# reshape feature matrix\n","features = np.array(features)\n","vector = np.array(vector)\n","vector = vector.reshape(vector.shape[0], 1)\n","\n","# Split the data and labels into train and test set\n","X_train, X_test, y_train, y_test = train_test_split(features, vector, test_size=0.2, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWSeE5rnL9wZ","colab_type":"code","colab":{}},"source":["# train model with SVM\n","# call LinearSVC\n","# train SVM\n","# call clf.predict\n","svm = LinearSVC()\n","svm.fit(X_train, y_train)\n","predictedValues = svm.predict(X_test)\n","acc = accuracy_score(y_test, predictedValues)*100\n","\n","# ===== Output functions ======\n","print('estimated labels: ', predictedValues)\n","print('ground truth labels: ', y_test)\n","print('Accuracy: ', acc, '%')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"U_FmkJhWe55I","colab_type":"text"},"source":["## Submission guidelines\n","---\n","Please submit a pdf file that includes a ***google shared link***(explained in the next paragraph) through blackboard. This pdf file should be named as **Surname_Givenname_SBUID_hw*.pdf** (example: Jordan_Michael_111134567_hw2.pdf for this assignment).\n","\n","To generate the ***google shared link***, first create a folder named ***Surname_Givenname_SBUID_hw*.*** (example: Jordan_Michael_111134567_hw2 for this assignment) in your Google Drive with your Stony Brook account. The structure of the files in the folder should be exactly the same as the one you downloaded. For instance in this homework:\n","\n","```\n","Jordan_Michael_111134567_hw2\n","        |---SourceImages\n","        |---Jordan_Michael_111134567_hw2.ipynb\n","        |---Jordan_Michael_111134567_hw2.pdf\n","```\n","Note that this folder should be in your Google Drive with your Stony Brook account.\n","\n","Then right click this folder, click ***Get shareable link (with edit access)***, in the People textfield, enter TA's email: ***kumara.kahatapitiya@stonybrook.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n","\n","Note that in google colab, we will only grade the version of the code right before the timestamp of the submission made in blackboard. \n","\n","Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_SBUID_hw2' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs.\n","\n","\n","-- DO NOT change the folder structure, please just fill in the blanks. <br>\n","\n","You are encouraged to post and answer questions on Piazza. Based on the amount of email that we have received in past years, there might be dealys in replying to personal emails. Please ask questions on Piazza and send emails only for personal issues.\n","\n","If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n","\n","Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n","\n","Late submission penalty: <br>\n","There will be a 10% penalty per day for late submission. However, you will have 4 days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period.\n"]},{"cell_type":"code","metadata":{"id":"1Gcc648Yw9wS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}